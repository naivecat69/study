#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DL.test.py (Ï†ïÍ∑úÌôî + Ïª§Î¶¨ÌÅòÎüº Stage1/2 ÌÜµÌï© Ìå®ÏπòÌåê)

Í∏∞Îä• ÏöîÏïΩ
- Ìï©ÏÑ± CW ÌïôÏäµ (--train_synth)
- ÎùºÎ≤® CSV ÌååÏù∏ÌäúÎãù (--finetune_csv)
- Ï∂îÎ°† (--audio_in)
- Ìè¥Îçî ÏùòÏÇ¨ÎùºÎ≤® ÏÉùÏÑ± (--make_pseudo)
- Ï†ïÍ∑úÌôî ÌååÏù¥ÌîÑÎùºÏù∏ (--normalize, --heterodyne, --bp_pre, --tnorm)
- Ïª§Î¶¨ÌÅòÎüº:
  * --stage s1 : ÌÜ†ÌÅ∞(DOT/DASH/SEP_ELEM/SEP_CHAR/SEP_WORD) CTC ÌïôÏäµ/Ï∂îÎ°†
  * --stage s2 : s1Ïùò ÏòàÏ∏° ÌÜ†ÌÅ∞Ïó¥ÏùÑ Î£©ÏóÖÏúºÎ°ú Î¨∏ÏûêÎ°ú ÏπòÌôòÌïòÏó¨ Ï∂úÎ†•(ÌõÑÏ≤òÎ¶¨)

ÌïÑÏöî Ìå®ÌÇ§ÏßÄ:
  pip install torch torchaudio librosa soundfile numpy scipy
"""

import os, math, random, argparse, string, glob, csv
from dataclasses import dataclass, replace
from typing import List, Tuple
import numpy as np
import librosa
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from scipy.signal import butter, lfilter, welch

# Í≥†Ï†ï ÏãúÎìú
SEED = 2025
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)

# ========================= Í∏∞Î≥∏ ÏÑ§Ï†ï =========================
@dataclass
class Config:
    # Ïò§ÎîîÏò§/ÌäπÏßï
    sr: int = 16000
    n_mels: int = 64
    hop_ms: float = 10.0
    win_ms: float = 25.0
    fmin: int = 40
    fmax: int = 4000
    pre_emph: float = 0.0
    # Ï†ïÍ∑úÌôî ÌîåÎûòÍ∑∏
    normalize: bool = True        # Ï†ÑÏ≤òÎ¶¨ ÌååÏù¥ÌîÑÎùºÏù∏ on/off
    heterodyne: bool = False      # f0 ÌïòÌñ•Î≥ÄÌôò(Ìó§ÌÖåÎ°úÎã§Ïù∏) ÏÇ¨Ïö©
    bp_pre: bool = True           # Ï¢ÅÏùÄ ÎåÄÏó≠ÌÜµÍ≥º ÏÇ¨Ïö©
    bp_bw_hz: int = 150
    tnorm: bool = False           # ÏÜçÎèÑ Ï†ïÍ∑úÌôî(dit Í∏∏Ïù¥ ÌëúÏ§ÄÌôî)
    # Ìï©ÏÑ±
    synth_wpm_min: int = 14
    synth_wpm_max: int = 28
    synth_f0_min: int = 300
    synth_f0_max: int = 1200
    atk_ms: float = 5.0
    dcy_ms: float = 5.0
    snr_db_min: float = 10.0
    snr_db_max: float = 35.0
    # ÌïôÏäµ
    batch_size: int = 16
    epochs: int = 10
    lr: float = 3e-4
    # Î™®Îç∏
    hidden: int = 128
    cnn_ch: int = 64

# ========================= Î™®Ïä§ ÌÖåÏù¥Î∏î/ÏÇ¨Ï†Ñ =========================
MORSE = {
    'A':'.-','B':'-...','C':'-.-.','D':'-..','E':'.','F':'..-.','G':'--.','H':'....','I':'..',
    'J':'.---','K':'-.-','L':'.-..','M':'--','N':'-.','O':'---','P':'.--.','Q':'--.-','R':'.-.',
    'S':'...','T':'-','U':'..-','V':'...-','W':'.--','X':'-..-','Y':'-.--','Z':'--..',
    '0':'-----','1':'.----','2':'..---','3':'...--','4':'....-','5':'.....',
    '6':'-....','7':'--...','8':'---..','9':'----.',
    '.':'.-.-.-', ',':'--..--', '?':'..--..', '/':'-..-.', '-':'-....-',
    ' ':' '
}
# Î¨∏Ïûê Ïù∏ÏãùÏö© ÏïåÌååÎ≤≥
VOCAB_CHARS = list(string.ascii_uppercase + string.digits + " .,?/-")

# Stage1 ÌÜ†ÌÅ∞ ÏßëÌï©
S1_TOKENS = ["DOT","DASH","SEP_ELEM","SEP_CHAR","SEP_WORD"]  # + BLANK

# -------------------- ÌÖçÏä§Ìä∏‚ÜîÌÜ†ÌÅ∞ Î≥ÄÌôò --------------------
def text_to_s1_tokens(text: str) -> List[str]:
    """Î¨∏ÏûêÏó¥(Í≥µÎ∞± Ìè¨Ìï®)ÏùÑ Stage1 ÌÜ†ÌÅ∞Ïó¥Î°ú Î≥ÄÌôò"""
    toks = []
    for idx, ch in enumerate(text.upper()):
        if ch == ' ':
            toks.append("SEP_WORD")
            continue
        code = MORSE.get(ch)
        if not code:
            continue
        for i, s in enumerate(code):
            toks.append("DOT" if s=='.' else "DASH")
            if i < len(code)-1:
                toks.append("SEP_ELEM")
        # Î¨∏Ïûê ÎÅù Í∞ÑÍ≤©(Îã®Ïñ¥ ÏÇ¨Ïù¥ Í≥µÎ∞±Ïù¥ ÏïÑÎãàÎ©¥)
        # Îã§Ïùå Î¨∏ÏûêÍ∞Ä Í≥µÎ∞±Ïù¥Í±∞ÎÇò Î¨∏Ïû•Ïùò ÎÅùÏù¥Î©¥ SEP_WORD/Î¨¥Ïãú, ÏïÑÎãàÎ©¥ SEP_CHAR
        if idx < len(text)-1 and text[idx+1] != ' ':
            toks.append("SEP_CHAR")
        elif idx < len(text)-1 and text[idx+1] == ' ':
            # Îã§Ïùå Î£®ÌîÑÏóêÏÑú SEP_WORDÍ∞Ä Îì§Ïñ¥Í∞ÄÎØÄÎ°ú ÏÉùÎûµ
            pass
    # Ïó∞ÏÜç SEP Ï†úÍ±∞/Ï†ïÎ¶¨
    cleaned=[]
    prev=None
    for t in toks:
        if t.startswith("SEP") and prev and prev.startswith("SEP"):
            if prev=="SEP_WORD":  # Îã®Ïñ¥ Íµ¨Î∂ÑÏù¥ ÏµúÏö∞ÏÑ†
                continue
        cleaned.append(t); prev=t
    return cleaned

def s1_tokens_to_text(tokens: List[str]) -> str:
    """Stage1 ÌÜ†ÌÅ∞Ïó¥ÏùÑ Î¨∏ÏûêÎ°ú Î≥µÏõê"""
    # ÌÜ†ÌÅ∞ÏùÑ dot/dash ÏãúÌÄÄÏä§Î°ú ÎàÑÏ†ÅÌïòÎã§Í∞Ä SEP_CHAR/SEP_WORDÏóêÏÑú Ïª§Î∞ã
    inv = {v:k for k,v in MORSE.items() if k!=' '}
    out=[]
    cur=[]
    for t in tokens:
        if t == "DOT": cur.append('.')
        elif t == "DASH": cur.append('-')
        elif t == "SEP_ELEM":
            # Í∞ôÏùÄ Î¨∏Ïûê ÎÇ¥Î∂Ä Íµ¨Î∂ÑÏûê ‚Üí ÏïÑÎ¨¥ Í≤ÉÎèÑ Ïïà Ìï®
            pass
        elif t == "SEP_CHAR":
            code=''.join(cur); cur=[]
            if code in inv: out.append(inv[code])
        elif t == "SEP_WORD":
            code=''.join(cur); cur=[]
            if code in inv: out.append(inv[code])
            out.append(' ')
    # ÏûîÏó¨ Ïª§Î∞ã
    if cur:
        code=''.join(cur)
        if code in inv: out.append(inv[code])
    # Í≥µÎ∞± Ï†ïÎ¶¨
    s=''.join(out)
    return ' '.join(s.split())

# ========================= Ìï©ÏÑ±/Ï†ÑÏ≤òÎ¶¨/ÌäπÏßï =========================
def synth_morse(text: str, fs: int, f0: float, wpm: float,
                atk: float=0.005, dcy: float=0.005, amp: float=0.9) -> np.ndarray:
    T = 1.2 / wpm
    def tone(dur):
        n = int(round(dur*fs)); t = np.arange(n)/fs
        return np.sin(2*np.pi*f0*t)
    def ad_env(n):
        a = int(max(1, round(atk*fs))); d = int(max(1, round(dcy*fs)))
        core = max(1, n-a-d)
        return np.concatenate([np.linspace(0,1,a,endpoint=False),
                               np.ones(core),
                               np.linspace(1,0,d,endpoint=True)])[:n]
    out=[]
    for ch in text.upper():
        if ch==' ':
            out.append(np.zeros(int(round(7*T*fs)))); continue
        code = MORSE.get(ch)
        if not code: continue
        for s in code:
            dur = T if s=='.' else 3*T
            seg = tone(dur)*ad_env(int(round(dur*fs)))*amp
            out.append(seg); out.append(np.zeros(int(round(T*fs))))
        out.append(np.zeros(int(round(2*T*fs))))
    return (np.concatenate(out) if out else np.zeros(0)).astype(np.float32)

def add_awgn(x: np.ndarray, snr_db: float) -> np.ndarray:
    if len(x)==0: return x
    ps = np.mean(x**2)+1e-12
    pn = ps / (10**(snr_db/10))
    n = np.sqrt(pn)*np.random.randn(*x.shape)
    return (x+n).astype(np.float32)

def pre_emphasis(x: np.ndarray, pre: float) -> np.ndarray:
    if pre<=0: return x
    y = np.copy(x); y[1:] = x[1:] - pre*x[:-1]; return y

def welch_peak_hz(x: np.ndarray, sr: int, fmin=80, fmax=4000) -> float:
    f,P = welch(x, fs=sr, nperseg=4096)
    m = (f>=fmin)&(f<=min(fmax, sr/2-100))
    return float(f[m][np.argmax(P[m])]) if np.any(m) else 600.0

def butter_bandpass(lowcut, highcut, fs, order=4):
    return butter(order, [lowcut/(fs/2), highcut/(fs/2)], btype='band')

def apply_bandpass(x: np.ndarray, sr: int, center: float, bw_hz: float) -> np.ndarray:
    low = max(20.0, center-bw_hz/2)
    high = min(sr/2-50.0, center+bw_hz/2)
    if low>=high: return x
    b,a = butter_bandpass(low, high, sr, order=4)
    return lfilter(b,a,x).astype(np.float32)

def heterodyne_env(x: np.ndarray, sr: int, f0: float, lp_ms: float=8.0) -> np.ndarray:
    """x * e^{-j2œÄ f0 t}Ïùò Ï†àÎåÄÍ∞íÏùÑ Ï†ÄÏó≠ÌèâÌôú ‚Üí Ìè¨ÎùΩÏÑ†"""
    t = np.arange(len(x))/sr
    cos = np.cos(2*np.pi*f0*t).astype(np.float32)
    sin = np.sin(2*np.pi*f0*t).astype(np.float32)
    i = x * cos
    q = x * (-sin)
    env = np.sqrt(i*i + q*q)
    n_lp = max(1, int(sr*(lp_ms/1000.0)))
    k = np.ones(n_lp, dtype=np.float32)/n_lp
    return np.convolve(env, k, mode='same').astype(np.float32)

def simple_envelope_after_bpf(x: np.ndarray, sr: int, f0: float, bw_hz:int=150, lp_ms:float=8.0)->np.ndarray:
    y = apply_bandpass(x, sr, f0, bw_hz)
    y = np.abs(y)
    n_lp=max(1, int(sr*(lp_ms/1000.0)))
    k=np.ones(n_lp, dtype=np.float32)/n_lp
    return np.convolve(y, k, mode='same').astype(np.float32)

def time_normalize_envelope(env: np.ndarray, sr: int, target_T_ms: float=50.0) -> np.ndarray:
    """dit Í∏∏Ïù¥ Ï∂îÏ†ï ÌõÑ ÏãúÍ∞ÑÏ∂ïÏùÑ ÌëúÏ§Ä dit Í∏∏Ïù¥Ïóê ÎßûÏ∂∞ Î¶¨ÏÉòÌîå"""
    # Ï†ÅÏùë ÏûÑÍ≥ÑÍ∞í
    thr = 0.5*(np.median(env) + 0.3*np.max(env))
    b = (env > thr).astype(np.int8)
    if len(b)==0: return env
    runs=[]; cur=b[0]; cnt=1
    for v in b[1:]:
        if v==cur: cnt+=1
        else: runs.append((cur,cnt)); cur=v; cnt=1
    runs.append((cur,cnt))
    on = [c for v,c in runs if v==1]
    if not on: return env
    T = max(1, int(np.median(on)))              # ÏÉòÌîå Ïàò
    T_ms = 1000.0 * T / sr
    alpha = (target_T_ms / T_ms) if T_ms>0 else 1.0
    target_sr = int(sr * alpha)
    if target_sr < 2000: target_sr = 2000       # ÎÑàÎ¨¥ ÎÇÆÏïÑÏßÄÏßÄ ÏïäÍ≤å
    out = librosa.resample(env, orig_sr=sr, target_sr=target_sr)
    # Îã§Ïãú ÏõêÎûò srÎ°ú Î¶¨ÏÉòÌîå(Í∏∏Ïù¥Îßå ÏÉÅÎåÄ ÌëúÏ§ÄÌôî Ìö®Í≥º Ïú†ÏßÄ)
    out2 = librosa.resample(out, orig_sr=target_sr, target_sr=sr)
    return out2.astype(np.float32)

def wav_to_logmel(wav: np.ndarray, sr: int, n_mels: int, hop_ms: float, win_ms: float,
                  fmin: int, fmax: int) -> np.ndarray:
    hop = int(round(sr*hop_ms/1000))
    win = int(round(sr*win_ms/1000))
    n_fft = max(512, 2**int(math.ceil(math.log2(max(256, win)))))
    S = librosa.feature.melspectrogram(y=wav, sr=sr, n_fft=n_fft,
                                       hop_length=hop, win_length=win,
                                       n_mels=n_mels, fmin=fmin, fmax=fmax, power=2.0)
    logmel = np.log1p(S).T
    m = np.mean(logmel,0,keepdims=True); s = np.std(logmel,0,keepdims=True)+1e-6
    return ((logmel-m)/s).astype(np.float32)

def load_audio_any(path: str, sr: int) -> np.ndarray:
    wav,_ = librosa.load(path, sr=sr, mono=True)
    return wav.astype(np.float32)

# ========================= ÏÇ¨Ï†Ñ/Î™®Îç∏ =========================
class Vocab:
    def __init__(self, tokens: List[str]):
        self.chars = tokens[:]
        self.blank_id = len(self.chars)
        self.stoi = {c:i for i,c in enumerate(self.chars)}
        self.itos = {i:c for i,c in enumerate(self.chars)}
        self.itos[self.blank_id] = ''  # CTC blank
    def text2ids(self, seq: List[str]) -> List[int]:
        return [self.stoi[c] for c in seq if c in self.stoi]
    def ids2seq_ctc(self, ids: List[int]) -> List[str]:
        out, prev = [], None
        for i in ids:
            if i==self.blank_id: prev=i; continue
            tok = self.itos.get(i,'')
            if i!=prev: out.append(tok)
            prev=i
        return out

class CWCTC(nn.Module):
    def __init__(self, n_mels=64, n_class=38, hidden=128, cnn_ch=64):
        super().__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(1, cnn_ch, 3, padding=1), nn.ReLU(), nn.MaxPool2d((1,2)),
            nn.Conv2d(cnn_ch, cnn_ch, 3, padding=1), nn.ReLU(), nn.MaxPool2d((1,2))
        )
        proj_in = cnn_ch * (n_mels//4)
        self.proj = nn.Linear(proj_in, hidden)
        self.rnn  = nn.LSTM(hidden, hidden, num_layers=2, bidirectional=True, batch_first=True)
        self.out  = nn.Linear(hidden*2, n_class)
    def forward(self, x):            # x: (B,1,T,M)
        z = self.cnn(x)              # (B,C,T,M')
        z = z.permute(0,2,1,3)       # (B,T,C,M')
        B,T,C,Mp = z.shape
        z = z.reshape(B,T,C*Mp)      # (B,T,C*M')
        z = self.proj(z)             # (B,T,H)
        z,_ = self.rnn(z)            # (B,T,2H)
        return self.out(z)           # (B,T,C)

# ========================= Îç∞Ïù¥ÌÑ∞ÏÖã/ÏΩúÎ†àÏù¥Ìä∏ =========================
def normalize_pipeline(wav: np.ndarray, cfg: Config) -> Tuple[np.ndarray, float]:
    """Ï†ïÍ∑úÌôî: f0 Ï∂îÏ†ï ‚Üí (Ìó§ÌÖåÎ°úÎã§Ïù∏ or BPF) ‚Üí Î†àÎ≤® ‚Üí (ÏÑ†ÌÉù) ÏÜçÎèÑ Ï†ïÍ∑úÌôî"""
    if not cfg.normalize:
        mx=np.max(np.abs(wav))+1e-9
        return (wav/mx).astype(np.float32), 0.0
    f0 = welch_peak_hz(wav, cfg.sr, fmin=cfg.fmin, fmax=cfg.fmax)
    if cfg.heterodyne:
        env = heterodyne_env(wav, cfg.sr, f0)
    else:
        env = simple_envelope_after_bpf(wav, cfg.sr, f0, bw_hz=cfg.bp_bw_hz)
    if cfg.tnorm:
        env = time_normalize_envelope(env, cfg.sr, target_T_ms=50.0)
    # Î©ú ÌäπÏßïÏùÄ env ÎåÄÏã† ÏõêÌååÌòï Í∏∞Î∞òÎèÑ Í∞ÄÎä•ÌïòÏßÄÎßå, Ïó¨Í∏∞ÏÑ† envÎ•º ÌëúÏ§ÄÌôî ÏûÖÎ†•ÏúºÎ°ú ÏÇ¨Ïö©
    mx = np.max(np.abs(env)) + 1e-9
    return (env/mx).astype(np.float32), f0

class SynthDatasetS1(Dataset):
    """Stage1 ÌÜ†ÌÅ∞ ÎùºÎ≤® Ìï©ÏÑ± Îç∞Ïù¥ÌÑ∞ÏÖã"""
    def __init__(self, cfg: Config, vocab: Vocab, n_items=2000, min_len=10, max_len=40):
        self.cfg=cfg; self.vocab=vocab; self.n=n_items; self.min_len=min_len; self.max_len=max_len
        self.alpha = VOCAB_CHARS
    def _rand_text(self)->str:
        L = random.randint(self.min_len, self.max_len)
        chars=[]
        for _ in range(L):
            if random.random()<0.16: chars.append(' ')
            else: chars.append(random.choice(self.alpha))
        return ''.join(chars).strip()
    def __len__(self): return self.n
    def __getitem__(self, idx):
        txt = self._rand_text()
        f0  = random.uniform(self.cfg.synth_f0_min, self.cfg.synth_f0_max)
        wpm = random.uniform(self.cfg.synth_wpm_min, self.cfg.synth_wpm_max)
        wav = synth_morse(txt, fs=self.cfg.sr, f0=f0, wpm=wpm,
                          atk=self.cfg.atk_ms/1000.0, dcy=self.cfg.dcy_ms/1000.0)
        # ÎÖ∏Ïù¥Ï¶à/ÎìúÎ¶¨ÌîÑÌä∏
        if len(wav)>0 and random.random()<0.5:
            drift = 1.0 + np.linspace(0, random.uniform(-0.002,0.002), len(wav))
            wav = (wav*drift).astype(np.float32)
        snr = random.uniform(self.cfg.snr_db_min, self.cfg.snr_db_max)
        wav = add_awgn(wav, snr)
        wav = pre_emphasis(wav, self.cfg.pre_emph)

        # Ï†ïÍ∑úÌôî ÌååÏù¥ÌîÑÎùºÏù∏
        x_env, _ = normalize_pipeline(wav, self.cfg)

        # ÏñáÏùÄ Î©ú(ÎòêÎäî envÎ•º Î©úÏ≤òÎüº Ï∑®Í∏â) ‚Äì Í∞ÑÎã®Ìôî: Î©ú ÏÉùÏÑ±Ïóê env ÏÇ¨Ïö©
        mel = wav_to_logmel(x_env, self.cfg.sr, self.cfg.n_mels, self.cfg.hop_ms, self.cfg.win_ms,
                            self.cfg.fmin, self.cfg.fmax)

        # Stage1 ÎùºÎ≤®(ÌÜ†ÌÅ∞)
        tokens = text_to_s1_tokens(txt)
        target = self.vocab.text2ids(tokens)
        return mel, np.array(target, np.int32), tokens

class LabeledAudioDatasetS1(Dataset):
    """CSV(path,text) ‚Üí Stage1 ÌÜ†ÌÅ∞ ÎùºÎ≤®Î°ú Î≥ÄÌôò ÌõÑ ÏÇ¨Ïö©"""
    def __init__(self, csv_path: str, cfg: Config, vocab: Vocab, shuffle=True):
        self.cfg=cfg; self.vocab=vocab
        rows=[]
        with open(csv_path, 'r', encoding='utf-8') as f:
            r = csv.reader(f)
            for row in r:
                if not row: continue
                if row[0].lower() in ('path','file','audio') and len(row)>=2: continue
                p = row[0].strip()
                t = ','.join(row[1:]).strip() if len(row)>1 else ''
                rows.append((p,t))
        if shuffle: random.shuffle(rows)
        self.rows=rows
    def __len__(self): return len(self.rows)
    def __getitem__(self, idx):
        p, raw = self.rows[idx]
        wav = load_audio_any(p, sr=self.cfg.sr)
        wav = pre_emphasis(wav, self.cfg.pre_emph)
        x_env, _ = normalize_pipeline(wav, self.cfg)
        mel = wav_to_logmel(x_env, self.cfg.sr, self.cfg.n_mels, self.cfg.hop_ms, self.cfg.win_ms,
                            self.cfg.fmin, self.cfg.fmax)
        tokens = text_to_s1_tokens(raw.upper())
        target = self.vocab.text2ids(tokens)
        return mel, np.array(target, np.int32), tokens

def collate_ctc(batch, blank_id: int):
    T_list=[b[0].shape[0] for b in batch]; M=batch[0][0].shape[1]; T_max=max(T_list)
    mels=np.zeros((len(batch),T_max,M),np.float32)
    in_lens=[]; targets=[]; tgt_lens=[]; texts=[]
    for i,(mel,tgt,txt) in enumerate(batch):
        t=mel.shape[0]; mels[i,:t,:]=mel
        in_lens.append(t); targets.append(tgt); tgt_lens.append(len(tgt)); texts.append(txt)
    flat_targets = np.concatenate(targets) if targets else np.zeros((0,),np.int32)
    mels = torch.from_numpy(mels).unsqueeze(1)
    flat_targets = torch.from_numpy(flat_targets).long()
    in_lens = torch.tensor(in_lens, dtype=torch.int32)
    tgt_lens= torch.tensor(tgt_lens,dtype=torch.int32)
    return mels, flat_targets, in_lens, tgt_lens, texts

# ========================= ÌïôÏäµ/ÌååÏù∏ÌäúÎãù/Ï∂îÎ°† =========================
def train_s1(cfg: Config, save_ckpt: str):
    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    vocab=Vocab(S1_TOKENS); n_class=len(vocab.chars)+1
    model=CWCTC(cfg.n_mels,n_class,cfg.hidden,cfg.cnn_ch).to(device)
    ds=SynthDatasetS1(cfg,vocab,n_items=2000)
    dl=DataLoader(ds,batch_size=cfg.batch_size,shuffle=True,
                  collate_fn=lambda b: collate_ctc(b, vocab.blank_id), num_workers=0)
    crit=nn.CTCLoss(blank=vocab.blank_id, zero_infinity=True)
    optim=torch.optim.AdamW(model.parameters(), lr=cfg.lr)
    for ep in range(1,cfg.epochs+1):
        model.train(); tot=0.0
        for mels,targets,in_lens,tgt_lens,_ in dl:
            mels=mels.to(device); targets=targets.to(device)
            in_lens=in_lens.to(device); tgt_lens=tgt_lens.to(device)
            logp = model(mels).log_softmax(-1).permute(1,0,2).contiguous()
            loss=crit(logp, targets, in_lens, tgt_lens)
            optim.zero_grad(); loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(),5.0); optim.step()
            tot+=float(loss.item())
        print(f"[S1 Epoch {ep}/{cfg.epochs}] CTC loss={tot/max(1,len(dl)):.4f}")
    os.makedirs(os.path.dirname(save_ckpt) or ".", exist_ok=True)
    torch.save({"model":model.state_dict(),"cfg":cfg.__dict__,"vocab":vocab.chars, "stage":"s1"}, save_ckpt)
    print(f"‚úÖ Saved checkpoint(S1): {save_ckpt}")

def finetune_s1(cfg: Config, ckpt_in: str, csv_path: str, save_ckpt: str,
                epochs=5, lr=1e-4, batch_size=8, freeze_cnn=False, freeze_rnn=False):
    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    ckpt=torch.load(ckpt_in, map_location=device)
    vocab=Vocab(ckpt.get("vocab", S1_TOKENS))
    n_class=len(vocab.chars)+1
    model=CWCTC(cfg.n_mels,n_class,cfg.hidden,cfg.cnn_ch).to(device)
    model.load_state_dict(ckpt["model"])
    if freeze_cnn: [setattr(p,'requires_grad',False) for p in model.cnn.parameters()]
    if freeze_rnn: [setattr(p,'requires_grad',False) for p in model.rnn.parameters()]
    ds=LabeledAudioDatasetS1(csv_path, cfg, vocab, shuffle=True)
    dl=DataLoader(ds,batch_size=batch_size,shuffle=True,
                  collate_fn=lambda b: collate_ctc(b, vocab.blank_id), num_workers=0)
    crit=nn.CTCLoss(blank=vocab.blank_id, zero_infinity=True)
    optim=torch.optim.AdamW(filter(lambda p:p.requires_grad, model.parameters()), lr=lr, weight_decay=1e-6)
    best=float('inf')
    for ep in range(1,epochs+1):
        model.train(); tot=0.0
        for mels,targets,in_lens,tgt_lens,_ in dl:
            mels=mels.to(device); targets=targets.to(device)
            in_lens=in_lens.to(device); tgt_lens=tgt_lens.to(device)
            logp=model(mels).log_softmax(-1).permute(1,0,2).contiguous()
            loss=crit(logp, targets, in_lens, tgt_lens)
            optim.zero_grad(); loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(),5.0); optim.step()
            tot+=float(loss.item())
        avg=tot/max(1,len(dl)); print(f"[S1 FT {ep}/{epochs}] loss={avg:.4f}")
        if avg<best:
            best=avg
            torch.save({"model":model.state_dict(),"cfg":cfg.__dict__,"vocab":vocab.chars, "stage":"s1"}, save_ckpt)
            print(f"  ‚Ü≥ ‚úÖ improved; saved: {save_ckpt}")

def infer_s1_to_tokens(cfg: Config, ckpt_path: str, audio_path: str) -> List[str]:
    """Ïò§ÎîîÏò§ ‚Üí Stage1 ÌÜ†ÌÅ∞Ïó¥"""
    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    ckpt=torch.load(ckpt_path, map_location=device)
    vocab=Vocab(ckpt.get("vocab", S1_TOKENS))
    n_class=len(vocab.chars)+1
    model=CWCTC(cfg.n_mels,n_class,cfg.hidden,cfg.cnn_ch).to(device)
    model.load_state_dict(ckpt["model"]); model.eval()

    wav=load_audio_any(audio_path, sr=cfg.sr)
    wav=pre_emphasis(wav, cfg.pre_emph)
    x_env, f0 = normalize_pipeline(wav, cfg)
    mel=wav_to_logmel(x_env,cfg.sr,cfg.n_mels,cfg.hop_ms,cfg.win_ms,cfg.fmin,cfg.fmax)
    with torch.no_grad():
        x=torch.from_numpy(mel).unsqueeze(0).unsqueeze(1).to(device)
        logits=model(x)
        pred=logits.argmax(-1).squeeze(0).tolist()
        tokens=vocab.ids2seq_ctc(pred)
    return tokens

# ----- DSP Î∞±ÏóÖ ÎîîÏΩîÎçî (Ï∞∏Í≥†Ïö©) -----
def dsp_morse_decode(wav: np.ndarray, sr: int, fmin=100, fmax=3000):
    f0=welch_peak_hz(wav,sr,fmin=fmin,fmax=fmax)
    x=apply_bandpass(wav,sr,center=f0,bw_hz=120); x=np.abs(x)
    n_lp=max(1,int(0.008*sr)); env=np.convolve(x, np.ones(n_lp)/n_lp, mode='same')
    thr=0.5*(np.median(env)+np.max(env)*0.3); b=(env>thr).astype(np.int8)
    runs=[]; cur=b[0]; cnt=1
    for v in b[1:]:
        if v==cur: cnt+=1
        else: runs.append((cur,cnt)); cur=v; cnt=1
    runs.append((cur,cnt))
    on=[c for v,c in runs if v==1]; off=[c for v,c in runs if v==0]
    if not on or not off: return ""
    T=max(1,int(np.median(on))); sym=[]
    for v,c in runs:
        if v==1: sym.append('.' if c<2.2*T else '-')
        else:
            if c<1.5*T: pass
            elif c<4.5*T: sym.append(' ')
            else: sym.append('   ')
    inv={v:k for k,v in MORSE.items() if k!=' '}
    out=[]; buf=[]
    for s in sym:
        if s in (' ','   '):
            code=''.join(buf); buf=[]
            if code in inv: out.append(inv[code])
            if s=='   ': out.append(' ')
        else:
            buf.append(s)
    if buf:
        code=''.join(buf)
        if code in inv: out.append(inv[code])
    return ''.join(out)

# ========================= ÏùòÏÇ¨ÎùºÎ≤® ÏÉùÏÑ± =========================
def decode_with_conf_s1(ckpt_path: str, audio_path: str, cfg: Config, bp_bw_hz: int=150):
    """S1 ÌÜ†ÌÅ∞ Í∏∞Ï§Ä confidence(ÌîÑÎ†àÏûÑ max ÌèâÍ∑†; blank Ï†úÏô∏)ÏôÄ ÌÜ†ÌÅ∞Ïó¥‚ÜíÎ¨∏Ïûê Î≥ÄÌôò ÌÖçÏä§Ìä∏ Î∞òÌôò"""
    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    ckpt=torch.load(ckpt_path, map_location=device); vocab=Vocab(ckpt.get("vocab", S1_TOKENS))
    n_class=len(vocab.chars)+1
    model=CWCTC(cfg.n_mels,n_class,cfg.hidden,cfg.cnn_ch).to(device)
    model.load_state_dict(ckpt["model"]); model.eval()

    wav=load_audio_any(audio_path, sr=cfg.sr)
    wav=pre_emphasis(wav, cfg.pre_emph)
    x_env,_ = normalize_pipeline(wav, cfg)
    mel=wav_to_logmel(x_env, cfg.sr, cfg.n_mels, cfg.hop_ms, cfg.win_ms, cfg.fmin, cfg.fmax)
    with torch.no_grad():
        x=torch.from_numpy(mel).unsqueeze(0).unsqueeze(1).to(device)
        logits=model(x)
        prob=logits.softmax(-1)[0].cpu()
        pred_ids=prob.argmax(-1).numpy().tolist()
        blank_id=len(vocab.chars)
        mask=np.array([i!=blank_id for i in pred_ids])
        conf=float(prob.max(-1).values.numpy()[mask].mean()) if mask.any() else 0.0
        tokens=vocab.ids2seq_ctc(pred_ids)
    text = s1_tokens_to_text(tokens)
    return text, conf

def make_pseudo_csv(cfg: Config, ckpt: str, in_dir: str, out_csv: str,
                    conf_th: float=0.75, agree_only: bool=False):
    files=[]
    for ext in ("*.mp3","*.wav","*.flac","*.ogg"):
        files += glob.glob(os.path.join(in_dir, ext))
    files.sort()
    kept=0; rows=[]
    print(f"[make_pseudo] scan {len(files)} files ...")
    for i,path in enumerate(files,1):
        try:
            text, conf = decode_with_conf_s1(ckpt, path, cfg)
            if not text or conf < conf_th:
                continue
            if agree_only:
                wav=load_audio_any(path, sr=cfg.sr)
                dsp_txt=dsp_morse_decode(wav, cfg.sr, fmin=cfg.fmin, fmax=cfg.fmax)
                if (dsp_txt or "").strip() != text.strip():
                    continue
            rows.append([path, text]); kept+=1
        except Exception:
            pass
        if i%20==0:
            print(f"  processed {i}/{len(files)} kept={kept}")
    os.makedirs(os.path.dirname(out_csv) or ".", exist_ok=True)
    with open(out_csv,"w",newline="",encoding="utf-8") as f:
        w=csv.writer(f); w.writerow(["path","text"]); w.writerows(rows)
    print(f"[make_pseudo] kept {kept} / {len(files)} ‚Üí {out_csv}")

# ========================= CLI/Î©îÏù∏ =========================
def parse_args():
    ap=argparse.ArgumentParser(description="CW(Morse) Ï†ïÍ∑úÌôî + Ïª§Î¶¨ÌÅòÎüº(Stage1/2) ÌååÏù¥ÌîÑÎùºÏù∏")
    # ÌäπÏßï/Ïò§ÎîîÏò§
    ap.add_argument("--sr", type=int, default=16000)
    ap.add_argument("--n_mels", type=int, default=64)
    ap.add_argument("--hop_ms", type=float, default=10.0)
    ap.add_argument("--win_ms", type=float, default=25.0)
    ap.add_argument("--fmin", type=int, default=40)
    ap.add_argument("--fmax", type=int, default=4000)
    ap.add_argument("--pre_emph", type=float, default=0.0)
    # Ï†ïÍ∑úÌôî
    ap.add_argument("--normalize", action="store_true", help="Ï†ïÍ∑úÌôî ÌååÏù¥ÌîÑÎùºÏù∏ ÏÇ¨Ïö©")
    ap.add_argument("--heterodyne", action="store_true", help="f0 ÌïòÌñ•Î≥ÄÌôò(Ìè¨ÎùΩÏÑ†)")
    ap.add_argument("--bp_pre", action="store_true", help="Ï¢ÅÏùÄ ÎåÄÏó≠ÌÜµÍ≥º Í∏∞Î∞ò Ìè¨ÎùΩÏÑ†")
    ap.add_argument("--bp_bw_hz", type=int, default=150)
    ap.add_argument("--tnorm", action="store_true", help="ÏÜçÎèÑ(dit) Ï†ïÍ∑úÌôî")
    # Ïª§Î¶¨ÌÅòÎüº
    ap.add_argument("--stage", choices=["s1","s2"], default="s1", help="s1=ÌÜ†ÌÅ∞ CTC, s2=ÌÜ†ÌÅ∞‚ÜíÎ¨∏Ïûê ÌõÑÏ≤òÎ¶¨")
    # Ìï©ÏÑ± ÌïôÏäµ
    ap.add_argument("--train_synth", action="store_true")
    ap.add_argument("--epochs", type=int, default=10)
    ap.add_argument("--batch_size", type=int, default=16)
    ap.add_argument("--save_ckpt", type=str, default="cw_s1.pt")
    # ÌååÏù∏ÌäúÎãù
    ap.add_argument("--load_ckpt", type=str, help="Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ΩÎ°ú (s1)")
    ap.add_argument("--finetune_csv", type=str, help="ÎùºÎ≤® CSV path,text (Î¨∏Ïûê ÎùºÎ≤®)")
    ap.add_argument("--finetune_epochs", type=int, default=5)
    ap.add_argument("--finetune_lr", type=float, default=1e-4)
    ap.add_argument("--finetune_bs", type=int, default=8)
    ap.add_argument("--freeze_cnn", action="store_true")
    ap.add_argument("--freeze_rnn", action="store_true")
    # Ï∂îÎ°†
    ap.add_argument("--audio_in", type=str, help="Ï∂îÎ°† Ïò§ÎîîÏò§(mp3/wav)")
    ap.add_argument("--out_txt", type=str, default=None)
    # ÏùòÏÇ¨ÎùºÎ≤®
    ap.add_argument("--make_pseudo", action="store_true")
    ap.add_argument("--in_dir", type=str)
    ap.add_argument("--out_csv", type=str, default="pseudo.csv")
    ap.add_argument("--conf_th", type=float, default=0.75)
    ap.add_argument("--agree_only", action="store_true")
    return ap.parse_args()

def main():
    args=parse_args()
    cfg=Config(sr=args.sr, n_mels=args.n_mels, hop_ms=args.hop_ms, win_ms=args.win_ms,
               fmin=args.fmin, fmax=args.fmax, pre_emph=args.pre_emph,
               normalize=args.normalize, heterodyne=args.heterodyne,
               bp_pre=args.bp_pre or (not args.heterodyne and args.normalize),
               bp_bw_hz=args.bp_bw_hz, tnorm=args.tnorm,
               batch_size=args.batch_size, epochs=args.epochs)

    did=False

    # 1) Ìï©ÏÑ± ÌïôÏäµ (Stage1)
    if args.train_synth:
        if args.stage!="s1":
            print("[WARN] Ìï©ÏÑ±ÌïôÏäµÏùÄ Stage1 ÌÜ†ÌÅ∞ CTC Í∏∞Ï§ÄÏúºÎ°ú ÎèôÏûëÌï©ÎãàÎã§. --stage s1 Í∂åÏû•.")
        train_s1(cfg, args.save_ckpt); did=True

    # 2) ÌååÏù∏ÌäúÎãù (CSV Î¨∏Ïûê ÎùºÎ≤® ‚Üí Stage1 ÌÜ†ÌÅ∞ÏúºÎ°ú Î≥ÄÌôòÌïòÏó¨ ÌïôÏäµ)
    if args.load_ckpt and args.finetune_csv:
        finetune_s1(cfg, args.load_ckpt, args.finetune_csv,
                    save_ckpt=args.save_ckpt or "cw_s1_ft.pt",
                    epochs=args.finetune_epochs, lr=args.finetune_lr,
                    batch_size=args.finetune_bs,
                    freeze_cnn=args.freeze_cnn, freeze_rnn=args.freeze_rnn)
        did=True

    # 3) Ï∂îÎ°† (s1: ÌÜ†ÌÅ∞Ïó¥ Ï∂úÎ†•, s2: Î¨∏Ïûê Ï∂úÎ†•)
    if args.load_ckpt and args.audio_in:
        tokens = infer_s1_to_tokens(cfg, args.load_ckpt, args.audio_in)
        if args.stage=="s2":
            text = s1_tokens_to_text(tokens)
        else:
            text = ' '.join(tokens)
        print("----- DECODED -----")
        print(text)
        print("-------------------")
        if args.out_txt:
            with open(args.out_txt,"w",encoding="utf-8") as f: f.write(text)
            print(f"üíæ Saved: {args.out_txt}")
        # DSP Ï∞∏Í≥† Ï∂úÎ†•
        try:
            wav=load_audio_any(args.audio_in, sr=cfg.sr)
            dsp_txt=dsp_morse_decode(wav, cfg.sr, fmin=cfg.fmin, fmax=cfg.fmax)
            if dsp_txt:
                print("----- DSP BASELINE -----"); print(dsp_txt); print("------------------------")
        except Exception:
            pass
        did=True

    # 4) ÏùòÏÇ¨ÎùºÎ≤® CSV ÏÉùÏÑ± (Î¨∏Ïûê ÎùºÎ≤® ÏÉùÏÑ±)
    if args.make_pseudo:
        if not args.load_ckpt:
            print("[ERROR] --make_pseudo Îäî --load_ckpt(Î≤†Ïù¥Ïä§ s1 Î™®Îç∏)Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.")
        elif not args.in_dir:
            print("[ERROR] --make_pseudo Îäî --in_dir(Ïò§ÎîîÏò§ Ìè¥Îçî)Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.")
        else:
            make_pseudo_csv(cfg, args.load_ckpt, args.in_dir, args.out_csv,
                            conf_th=args.conf_th, agree_only=args.agree_only)
        did=True

    if not did:
        print("Nothing to do.\n"
              "- Ìï©ÏÑ± ÌïôÏäµ(Stage1): --train_synth --normalize --bp_pre --epochs 10 --save_ckpt cw_s1.pt\n"
              "- ÌååÏù∏ÌäúÎãù(Stage1):  --load_ckpt cw_s1.pt --finetune_csv data.csv --normalize --bp_pre --save_ckpt cw_s1_ft.pt\n"
              "- Ï∂îÎ°†(ÌÜ†ÌÅ∞):        --load_ckpt cw_s1_ft.pt --audio_in input.mp3 --normalize --bp_pre --stage s1 --out_txt out.txt\n"
              "- Ï∂îÎ°†(Î¨∏Ïûê ÏπòÌôò):   --load_ckpt cw_s1_ft.pt --audio_in input.mp3 --normalize --bp_pre --stage s2 --out_txt out.txt\n"
              "- ÏùòÏÇ¨ÎùºÎ≤®:          --make_pseudo --load_ckpt cw_s1.pt --in_dir FOLDER --out_csv pseudo.csv --normalize --bp_pre")

if __name__ == "__main__":
    main()

# ============================ ÏÇ¨Ïö© ÏòàÏãú ============================
# (PowerShell)
#
# 0) VRAM Î∂ÄÏ°± Ïãú
#    $env:PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"
#
# 1) Ìï©ÏÑ± Îç∞Ïù¥ÌÑ∞Î°ú Stage1(ÌÜ†ÌÅ∞) Î≤†Ïù¥Ïä§ ÌïôÏäµ + Ï†ïÍ∑úÌôî ÏÇ¨Ïö©
#    python DL.test.py --train_synth --normalize --bp_pre --epochs 12 --batch_size 8 --save_ckpt cw_s1.pt
#
# 2) (ÏÑ†ÌÉù) ÏùòÏÇ¨ÎùºÎ≤® ÏÉùÏÑ± ‚Üí CSV (Î¨∏Ïûê ÎùºÎ≤® ÏûêÎèô)
#    python DL.test.py --make_pseudo --load_ckpt cw_s1.pt --in_dir C:\data\cw200 --out_csv C:\data\cw200\pseudo.csv --normalize --bp_pre --conf_th 0.75 --agree_only
#
# 3) Stage1 ÌååÏù∏ÌäúÎãù(Î¨∏Ïûê ÎùºÎ≤® CSVÎ•º ÌÜ†ÌÅ∞ÏúºÎ°ú ÎÇ¥Î∂Ä Î≥ÄÌôò)
#    python DL.test.py --load_ckpt cw_s1.pt --finetune_csv C:\data\cw200\pseudo.csv --normalize --bp_pre --finetune_epochs 3 --finetune_lr 1e-4 --finetune_bs 8 --save_ckpt cw_s1_ft.pt
#
# 4) Ï∂îÎ°†
#   (a) ÌÜ†ÌÅ∞Ïó¥Îßå Î≥¥Í≥† Ïã∂ÏùÑ Îïå:
#    python DL.test.py --load_ckpt cw_s1_ft.pt --audio_in feed.mp3 --normalize --bp_pre --stage s1 --out_txt out_tokens.txt
#   (b) ÏµúÏ¢Ö Î¨∏ÏûêÎ°ú Î≥¥Í≥† Ïã∂ÏùÑ Îïå(Í∂åÏû•):
#    python DL.test.py --load_ckpt cw_s1_ft.pt --audio_in feed.mp3 --normalize --bp_pre --stage s2 --out_txt out.txt
#
# 5) ÏòµÏÖò Î©îÎ™®
#   --heterodyne  : f0 ÌïòÌñ•Î≥ÄÌôò Ìè¨ÎùΩÏÑ†(Ï†ïÌôïÌïòÏßÄÎßå Ïó∞ÏÇ∞ Ï°∞Í∏à‚Üë)
#   --bp_pre      : Ï¢ÅÏùÄ ÎåÄÏó≠ÌÜµÍ≥º + Ìè¨ÎùΩÏÑ†(Îπ†Î•¥Í≥† ÌäºÌäº, Í∏∞Î≥∏ Ï∂îÏ≤ú)
#   --tnorm       : dit Í∏∏Ïù¥ ÌëúÏ§ÄÌôî(ÏÜçÎèÑ Î∂ÑÏÇ∞ ÌÅ¥ Îïå ÏºúÎ©¥ Ï¢ãÏùå)
#
